{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y_fk30kCvW9"
      },
      "source": [
        "# Sampling Energy Models\n",
        "\n",
        "**Author**: Chris Oswald\n",
        "\n",
        "**Course**: CS676/ECE689 Advanced Topics in Deep Learning (Spring 2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrgKMjtN0uA"
      },
      "source": [
        "## Question 2: Sampling from energy models with Langevin dynamics and stein scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAI5OQGsOAW1"
      },
      "source": [
        "Energy based models learn an energy functional $E_{\\theta}:\\mathcal{X}\\rightarrow \\mathbb{R}$. We look at the Gibbs distribution as follows:\n",
        "\n",
        "$p_{\\theta}(x) = \\frac{1}{Z_{\\theta}}e^{-E_{\\theta}(x)}$, where $Z_{\\theta} = \\int_{\\mathcal{X}}e^{-E_{\\theta}(y)}dy$.\n",
        "\n",
        "Directly sampling from $p_{\\theta}$ is hard, but we can approximate samples using a Markov chain with stationary distribution $p_{\\theta}$, spscifically, we have the discretized Langevin dynamics:\n",
        "\n",
        "<!-- '''$\\frac{d x_t}{dt} = \\nabla_x\\log p_{\\theta}(x_t)dt+\\sqrt{2}dW_t,$ -->\n",
        "\n",
        "<!-- where $dW_t$ is a white noise process, given by the Brownian motion $W_t$. -->\n",
        "\n",
        "<!-- (Diffusion following these dynamics converges asymptotically to samples $x_t\\sim p_{\\theta}$, in the sense that $D(x_t\\|p_{\\theta})→0$ as $t→∞$.) -->\n",
        "\n",
        "<!-- Discretizing the Langevin dynamics, we have -->\n",
        "\n",
        "$x_{t+1} = x_t-\\eta\\nabla_x\\log p_{\\theta}(x_t)+\\sqrt{2\\eta}\\epsilon_t,$\n",
        "\n",
        "where $\\epsilon_t\\sim\\mathcal{N}(0,I)$, $\\eta$ is the step size.\n",
        "\n",
        "We consider a 2D case, where $x\\in\\mathbb{R}^2$. Say $E_{\\theta}(x) = \\theta\\cdot x$, where $\\theta\\in\\mathbb{R}^{2}$ is a vector and has all the parameters.\n",
        "\n",
        "Calculate the expression for the distribution $x_N$, where $x_0\\sim \\mathcal{N}(0,I)$, and $N$ is the number of steps, in terms of $\\eta, \\theta, N$.\n",
        "\n",
        "(You can implement and see if your computational results match your analytical results. A helpful website: https://courses.cs.washington.edu/courses/cse599i/20au/resources/L16_ebm.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzfIzIk_osOl"
      },
      "source": [
        "## My Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrFwpYBufk8C"
      },
      "source": [
        "Consider the first step $\\mathbf{x_1}$:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\mathbf{x_1}\n",
        "    &=\n",
        "    \\mathbf{x_0} - \\eta \\nabla_x \\log(\\frac{1}{Z_\\theta} e^{-(\\mathbf{\\theta} \\cdot \\mathbf{x_0})} ) + \\sqrt{2\\eta} \\epsilon_{t_0} \\\\\n",
        "    &=\n",
        "    \\mathbf{x_0} - \\eta \\nabla_x\n",
        "    \\left[\\log(e^{-(\\mathbf{\\theta} \\cdot \\mathbf{x_0})} )\n",
        "    - \\log(Z_\\theta)\\right]\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_0} \\\\\n",
        "    &=\n",
        "    \\mathbf{x_0} - \\eta \\nabla_x\n",
        "    \\left[-(\\mathbf{\\theta} \\cdot \\mathbf{x_0})\n",
        "    - \\log(Z_\\theta)\\right]\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_0} \\\\\n",
        "    &=\n",
        "    \\mathbf{x_0} + \\eta \\nabla_x\n",
        "    \\left[(\\mathbf{\\theta} \\cdot \\mathbf{x_0})\n",
        "    \\right]\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_0} \\\\\n",
        "    &=\n",
        "    \\mathbf{x_0} + \\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_0} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE8QGPtmptrT"
      },
      "source": [
        "Thus, for any step $\\mathbf{x_N}$ where $N>1$, we have:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\mathbf{x_N}\n",
        "    &=\n",
        "    \\mathbf{x_{(N-1)}} + \\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_{(N-1)}} \\\\\n",
        "    &=\n",
        "    \\left[\\mathbf{x_{(N-2)}} + \\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_{(N-2)}} \\right]\n",
        "    + \\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\epsilon_{t_{(N-1)}}\\\\\n",
        "    &\n",
        "    \\vdots \\\\\n",
        "    &=\n",
        "    \\mathbf{x_{0}} + N *\\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\sum_{i=0}^{N-1}\\epsilon_{t_{i}} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\mathbf{\\epsilon_{t_{i}}} \\thicksim \\mathcal{N}\\big(0, I), \\quad i=0, \\dots, N-1\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\mathbf{x_0} \\thicksim \\mathcal{N}\\big(0, I)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez4bj6YUzWFo"
      },
      "source": [
        "Since the sum of (independent) standard normal random variables is also normal, we have\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\mathbf{x_N}\n",
        "    &=\n",
        "    N \\eta \\mathbf{\\theta}\n",
        "    + \\sqrt{2\\eta} \\mathcal{E} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\mathbf{\\mathcal{E}} \\thicksim \\mathcal{N}\\big(0, (N+1)I)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sWdQOQ821cu"
      },
      "source": [
        "### Computational Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7gRa0ZYnbun"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7yb9xz4kgyY"
      },
      "outputs": [],
      "source": [
        "# Define functions\n",
        "def gen_gaus_noise(ndim, scale=1):\n",
        "    return np.random.multivariate_normal([0]*ndim, scale * np.eye(ndim))\n",
        "\n",
        "def calc_analytic_solution(N, theta, ndim, lr):\n",
        "    return N * lr * theta + np.sqrt(2*lr) * gen_gaus_noise(ndim, scale=(N+1))\n",
        "\n",
        "def gen_next_sample(x0, theta, ndim, lr):\n",
        "    return x0 - lr * (-theta) + np.sqrt(2*lr) * gen_gaus_noise(ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aADNM_Ue243H",
        "outputId": "192e0ac0-4412-48dc-aa28-75db17393938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theta: [0.80342804 0.5275223 ]\n",
            "Analytical solution: [817.50706469 489.13108999]\n",
            "MC solution: [808.83338304 495.30474785]\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "np.random.seed(999)\n",
        "n_samples = 100000\n",
        "ndim = 2\n",
        "lr = 1e-2\n",
        "theta = np.array([np.random.rand(), np.random.rand()])\n",
        "print(f'Theta: {theta}')\n",
        "\n",
        "# Calculate analytical results\n",
        "analytic_solution = calc_analytic_solution(n_samples, theta, ndim, lr)\n",
        "\n",
        "# Calculate Markov Chain results\n",
        "samples = np.zeros((n_samples, ndim))\n",
        "current_sample = gen_gaus_noise(ndim)\n",
        "for i in range(n_samples):\n",
        "    next_sample = gen_next_sample(current_sample, theta, ndim, lr)\n",
        "    samples[i, :] = next_sample\n",
        "    current_sample = next_sample\n",
        "\n",
        "print(f'Analytical solution: {analytic_solution}')\n",
        "print(f'MC solution: {samples[-1, :]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References:\n",
        "\n",
        "- https://courses.cs.washington.edu/courses/cse599i/20au/resources/L16_ebm.pdf"
      ],
      "metadata": {
        "id": "p4-1b4S85fk1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}