{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y3XpY_hw1hb"
      },
      "source": [
        "# Energy-based Models\n",
        "\n",
        "**Author**: Chris Oswald\n",
        "\n",
        "**Course**: CS676/ECE689 Advanced Topics in Deep Learning (Spring 2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55BzFP065n9y"
      },
      "source": [
        "## Question 4: Energy-based Models\n",
        "\n",
        "- make a simple energy-based model (exp^-NN).\n",
        "- train and apply any of the Monte Carlo samplings methods on USPS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc1YM4Ah5n9y"
      },
      "source": [
        "References:\n",
        "\n",
        "- https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n",
        "- https://github.com/phlippe/uvadlc_notebooks/tree/master\n",
        "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "Cba-xUs4IJj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCIgUfb3YCHm",
        "outputId": "3ed44d9d-ba72-478d-de7c-fde97bd5682d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSLBTZOww1hh"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_dir = \"./drive/MyDrive/Colab Notebooks/Models\"\n",
        "os.makedirs(models_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "VRkI7f9aKCNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbqTTFeww1hf"
      },
      "source": [
        "## Imports and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lEjedRvw1hf"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "import normflows as nf\n",
        "\n",
        "import copy\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wddi9-NxFpg2",
        "outputId": "4c11b6cb-a5c6-4b86-d778-e120ca967690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2 to USPS/usps.bz2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6579383/6579383 [00:01<00:00, 3464054.87it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import USPS data (reshaped to 16x16 and normalized to range [-1, 1])\n",
        "ds_USPS  = torchvision.datasets.USPS(\n",
        "    'USPS',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(16),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, ), (0.5, )), # Scale to [-1, 1]\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdGgKpCrFpWL",
        "outputId": "a75244fe-690b-4676-ba92-128f6f0c2bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'uvadlc_notebooks'...\n",
            "remote: Enumerating objects: 4502, done.\u001b[K\n",
            "remote: Counting objects: 100% (513/513), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 4502 (delta 390), reused 437 (delta 356), pack-reused 3989\u001b[K\n",
            "Receiving objects: 100% (4502/4502), 183.25 MiB | 29.56 MiB/s, done.\n",
            "Resolving deltas: 100% (2976/2976), done.\n",
            "Attempting to import Deep_Energy_Models.Sampler using nbimporter\n",
            "Collecting nbimporter\n",
            "  Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
            "Installing collected packages: nbimporter\n",
            "Successfully installed nbimporter-0.3.4\n",
            "Attempting to import Deep_Energy_Models.Sampler using ipynb.fs.defs\n",
            "Collecting ipynb\n",
            "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: ipynb\n",
            "Successfully installed ipynb-0.5.1\n",
            "Attempting to import Deep_Energy_Models.Sampler using import_ipynb\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.9.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->import_ipynb)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.17.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->import_ipynb) (4.2.0)\n",
            "Installing collected packages: jedi, import_ipynb\n",
            "Successfully installed import_ipynb-0.1.4 jedi-0.19.1\n",
            "All attempts unsuccessful...using static source code.\n"
          ]
        }
      ],
      "source": [
        "# Import Sampler class from https://github.com/phlippe/uvadlc_notebooks/tree/master\n",
        "# Available under MIT license, Copyright (c) 2020 Phillip Lippe\n",
        "\n",
        "# Importing class directly from Jupyter notebook is no longer working consistently\n",
        "# (probably an issue on my end though)\n",
        "try:\n",
        "    os.chdir('/content')\n",
        "    !git clone https://github.com/phlippe/uvadlc_notebooks.git\n",
        "    os.chdir('/content/uvadlc_notebooks/docs/tutorial_notebooks/tutorial8')\n",
        "\n",
        "    print(f'Attempting to import Deep_Energy_Models.Sampler using nbimporter')\n",
        "    !pip install nbimporter\n",
        "    from Deep_Energy_Models import Sampler\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "    try:\n",
        "        print(f'Attempting to import Deep_Energy_Models.Sampler using ipynb.fs.defs')\n",
        "        !pip install ipynb\n",
        "        from ipynb.fs.defs.Deep_Energy_Models import Sampler\n",
        "\n",
        "    except SyntaxError:\n",
        "\n",
        "        try:\n",
        "            print(f'Attempting to import Deep_Energy_Models.Sampler using import_ipynb')\n",
        "            !pip install import_ipynb\n",
        "            from Deep_Energy_Models import Sampler\n",
        "\n",
        "        except:\n",
        "            print('All attempts unsuccessful...using static source code.')\n",
        "\n",
        "            \"\"\"\n",
        "            Original source code from\n",
        "            https://github.com/phlippe/uvadlc_notebooks/tree/master\n",
        "            MIT license, Copyright (c) 2020 Phillip Lippe\n",
        "            \"\"\"\n",
        "            import random\n",
        "\n",
        "            class Sampler:\n",
        "\n",
        "                def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
        "                    \"\"\"\n",
        "                    Inputs:\n",
        "                        model - Neural network to use for modeling E_theta\n",
        "                        img_shape - Shape of the images to model\n",
        "                        sample_size - Batch size of the samples\n",
        "                        max_len - Maximum number of data points to keep in the buffer\n",
        "                    \"\"\"\n",
        "                    super().__init__()\n",
        "                    self.model = model\n",
        "                    self.img_shape = img_shape\n",
        "                    self.sample_size = sample_size\n",
        "                    self.max_len = max_len\n",
        "                    self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
        "\n",
        "                def sample_new_exmps(self, steps=60, step_size=10):\n",
        "                    \"\"\"\n",
        "                    Function for getting a new batch of \"fake\" images.\n",
        "                    Inputs:\n",
        "                        steps - Number of iterations in the MCMC algorithm\n",
        "                        step_size - Learning rate nu in the algorithm above\n",
        "                    \"\"\"\n",
        "                    # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
        "                    n_new = np.random.binomial(self.sample_size, 0.05)\n",
        "                    rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
        "                    old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
        "                    inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
        "\n",
        "                    # Perform MCMC sampling\n",
        "                    inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
        "\n",
        "                    # Add new images to the buffer and remove old ones if needed\n",
        "                    self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
        "                    self.examples = self.examples[:self.max_len]\n",
        "                    return inp_imgs\n",
        "\n",
        "                @staticmethod\n",
        "                def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
        "                    \"\"\"\n",
        "                    Function for sampling images for a given model.\n",
        "                    Inputs:\n",
        "                        model - Neural network to use for modeling E_theta\n",
        "                        inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
        "                        steps - Number of iterations in the MCMC algorithm.\n",
        "                        step_size - Learning rate nu in the algorithm above\n",
        "                        return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
        "                    \"\"\"\n",
        "                    # Before MCMC: set model parameters to \"required_grad=False\"\n",
        "                    # because we are only interested in the gradients of the input.\n",
        "                    is_training = model.training\n",
        "                    model.eval()\n",
        "                    for p in model.parameters():\n",
        "                        p.requires_grad = False\n",
        "                    inp_imgs.requires_grad = True\n",
        "\n",
        "                    # Enable gradient calculation if not already the case\n",
        "                    had_gradients_enabled = torch.is_grad_enabled()\n",
        "                    torch.set_grad_enabled(True)\n",
        "\n",
        "                    # We use a buffer tensor in which we generate noise each loop iteration.\n",
        "                    # More efficient than creating a new tensor every iteration.\n",
        "                    noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
        "\n",
        "                    # List for storing generations at each step (for later analysis)\n",
        "                    imgs_per_step = []\n",
        "\n",
        "                    # Loop over K (steps)\n",
        "                    for _ in range(steps):\n",
        "                        # Part 1: Add noise to the input.\n",
        "                        noise.normal_(0, 0.005)\n",
        "                        inp_imgs.data.add_(noise.data)\n",
        "                        inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "                        # Part 2: calculate gradients for the current input.\n",
        "                        out_imgs = -model(inp_imgs)\n",
        "                        out_imgs.sum().backward()\n",
        "                        inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "\n",
        "                        # Apply gradients to our current samples\n",
        "                        inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
        "                        inp_imgs.grad.detach_()\n",
        "                        inp_imgs.grad.zero_()\n",
        "                        inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "                        if return_img_per_step:\n",
        "                            imgs_per_step.append(inp_imgs.clone().detach())\n",
        "\n",
        "                    # Reactivate gradients for parameters for training\n",
        "                    for p in model.parameters():\n",
        "                        p.requires_grad = True\n",
        "                    model.train(is_training)\n",
        "\n",
        "                    # Reset gradient calculation to setting before this function\n",
        "                    torch.set_grad_enabled(had_gradients_enabled)\n",
        "\n",
        "                    if return_img_per_step:\n",
        "                        return torch.stack(imgs_per_step, dim=0)\n",
        "                    else:\n",
        "                        return inp_imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFLGwB6WFpds"
      },
      "outputs": [],
      "source": [
        "# Specify convolutional neural network model architecture\n",
        "class SwishAF(nn.Module):\n",
        "    \"\"\"Swish activation function.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Based on: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "    and https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels: int, hidden_channels: int, out_dim: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_chan_1 = hidden_channels // 2\n",
        "        hidden_chan_2 = hidden_channels\n",
        "        hidden_chan_3 = hidden_channels * 2\n",
        "\n",
        "        self.cnn_seq = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, hidden_chan_1, kernel_size=5, padding=2), # [16x16]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_1, hidden_chan_1, kernel_size=5, padding=2), # [16x16]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_1, hidden_chan_2, kernel_size=5, padding=2), # [16x16]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_2, hidden_chan_2, kernel_size=5, padding=0), # [12x12]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_2, hidden_chan_2, kernel_size=5, padding=0), # [8x8]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_2, hidden_chan_3, kernel_size=5, padding=0), # [4x4]\n",
        "            SwishAF(),\n",
        "            nn.Conv2d(hidden_chan_3, hidden_chan_3, kernel_size=3, padding=0), # [2x2]\n",
        "            SwishAF(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_chan_3 * 2 * 2, hidden_chan_3),\n",
        "            SwishAF(),\n",
        "            nn.Linear(hidden_chan_3, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output = self.cnn_seq(x).squeeze(dim=-1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBCOPywTej5r"
      },
      "outputs": [],
      "source": [
        "# Specify energy model architecture\n",
        "class EnergyModel():\n",
        "    \"\"\"\n",
        "    Based on: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n",
        "    (Available under MIT license, Copyright (c) 2020 Phillip Lippe)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_shape, batch_size, lr=1e-4, **CNN_kwargs):\n",
        "        super().__init__()\n",
        "        self.cnn = CNN(**CNN_kwargs)\n",
        "        self.sampler = Sampler(self.cnn, img_shape, batch_size)\n",
        "        self.example_input_array = torch.zeros(1, *img_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.cnn(x)\n",
        "        return z\n",
        "\n",
        "    def compute_loss(self, real_imgs):\n",
        "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
        "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "        # Sample using MCMC sampler\n",
        "        fake_imgs = self.sampler.sample_new_exmps(steps=40, step_size=10) #USPS images are 16x16 compared to 28x28 MNIST\n",
        "\n",
        "        # Calculated predicted energy score\n",
        "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
        "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
        "\n",
        "        # Calculate loss\n",
        "        # regularized_loss = 0.1 * (real_out ** 2 + fake_out ** 2).mean()\n",
        "        contrastive_div_loss = fake_out.mean() - real_out.mean()\n",
        "        # loss = regularized_loss + contrastive_div_loss\n",
        "        return contrastive_div_loss\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.cnn.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.cnn.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yi2tYwseaNx"
      },
      "outputs": [],
      "source": [
        "# Create energy model\n",
        "img_shape = tuple(ds_USPS[0][0].shape)\n",
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "\n",
        "model = EnergyModel(\n",
        "    img_shape=img_shape,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    input_channels=img_shape[0],\n",
        "    hidden_channels=32,\n",
        "    out_dim=1\n",
        ")\n",
        "optimizer = optim.Adam(model.cnn.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UlKoXJNza7o",
        "outputId": "033cbb68-9dc6-4e7d-8186-1c4dec5618e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: -0.003738 [Epoch 1/20]\n",
            "Saved model for epoch 1\n",
            "Loss: -0.000868 [Epoch 2/20]\n",
            "Saved model for epoch 2\n",
            "Loss: 0.000140 [Epoch 3/20]\n",
            "Saved model for epoch 3\n",
            "Loss: 0.000124 [Epoch 4/20]\n",
            "Saved model for epoch 4\n",
            "Loss: -0.000141 [Epoch 5/20]\n",
            "Loss: -0.000370 [Epoch 6/20]\n",
            "Loss: -0.000151 [Epoch 7/20]\n",
            "Loss: 0.000059 [Epoch 8/20]\n",
            "Saved model for epoch 8\n",
            "Loss: -0.000017 [Epoch 9/20]\n",
            "Saved model for epoch 9\n",
            "Loss: 0.000259 [Epoch 10/20]\n",
            "Loss: -0.000383 [Epoch 11/20]\n",
            "Loss: 0.000125 [Epoch 12/20]\n",
            "Loss: -0.000194 [Epoch 13/20]\n",
            "Loss: -0.000046 [Epoch 14/20]\n",
            "Loss: 0.000307 [Epoch 15/20]\n",
            "Loss: -0.000242 [Epoch 16/20]\n",
            "Loss: 0.000044 [Epoch 17/20]\n",
            "Loss: -0.000119 [Epoch 18/20]\n",
            "Loss: -0.000055 [Epoch 19/20]\n",
            "Loss: 0.000134 [Epoch 20/20]\n"
          ]
        }
      ],
      "source": [
        "# Load/train model\n",
        "load_model = False\n",
        "load_model_name = \"\"\n",
        "load_model_path = os.path.join(models_dir, f\"{load_model_name}.pt\")\n",
        "\n",
        "if load_model:\n",
        "    if torch.cuda.is_available():\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "    else:\n",
        "        model.load_state_dict(\n",
        "            torch.load(load_model_path, map_location=torch.device(\"cpu\"))\n",
        "        )\n",
        "else:\n",
        "    epochs = 20\n",
        "\n",
        "    dl = DataLoader(ds_USPS, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    size = len(dl.dataset)\n",
        "\n",
        "    lowest_abs_loss = None # Note that we want abs(loss) to be close to 0 for this model\n",
        "\n",
        "    model.cnn.to(device)\n",
        "    model.cnn.train()\n",
        "    for t in range(epochs):\n",
        "        batch_losses = []\n",
        "        for batch_idx, (real_imgs, _) in enumerate(dl):\n",
        "            real_imgs = real_imgs.to(device)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model.compute_loss(real_imgs)\n",
        "\n",
        "            if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Record training batch loss\n",
        "                loss_val = loss.item()\n",
        "                batch_losses.append(loss_val)\n",
        "\n",
        "        epoch_avg_loss = np.mean(batch_losses)\n",
        "        print(f\"Loss: {epoch_avg_loss:>7f} [Epoch {t+1}/{epochs}]\")\n",
        "\n",
        "        # Save model if improving\n",
        "        if (lowest_abs_loss is None) or (abs(epoch_avg_loss) < lowest_abs_loss):\n",
        "            lowest_abs_loss = abs(epoch_avg_loss)\n",
        "            model.save(os.path.join(models_dir, f'EnergyModel_epoch{t+1}.pt'))\n",
        "            print(f'Saved model for epoch {t+1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgTkUAEJ20YM"
      },
      "outputs": [],
      "source": [
        "# Generate samples\n",
        "model.cnn.eval()\n",
        "n_samples = 1\n",
        "start_imgs = torch.rand((n_samples, ) + img_shape).to(device)\n",
        "start_imgs = start_imgs * 2 - 1 # Normalize to range [-1, 1]\n",
        "torch.set_grad_enabled(True) # Set to True for sampling\n",
        "imgs_per_step = Sampler.generate_samples(\n",
        "    model.cnn,\n",
        "    start_imgs,\n",
        "    steps=100,\n",
        "    step_size=10,\n",
        "    return_img_per_step=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "2TOjwwTqO9nd",
        "outputId": "5070e797-87d1-4ad7-8dc9-e26281450057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79a99a96e200>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generated image\n",
        "plt.imshow(imgs_per_step.squeeze()[-1].detach().to(\"cpu\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}